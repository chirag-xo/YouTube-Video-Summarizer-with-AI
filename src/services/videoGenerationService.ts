import { YouTubeVideoData, VideoSummaryData } from './youtubeService'

export interface VideoGenerationConfig {
  duration: number // Dynamic based on voiceover length
  width: number
  height: number
  fps: number
  voiceSettings: {
    voice: string
    speed: number
    pitch: number
  }
  visualStyle: {
    theme: 'modern' | 'minimal' | 'dynamic' | 'professional'
    primaryColor: string
    secondaryColor: string
    fontFamily: string
  }
}

export interface VideoSegment {
  startTime: number
  endTime: number
  type: 'intro' | 'keypoint' | 'highlight' | 'conclusion'
  content: {
    text: string
    voiceText: string
    visualElements: VisualElement[]
    originalVideoFrame?: string // Frame from original video
    transitions: TransitionEffect[]
  }
}

export interface VisualElement {
  type: 'text' | 'image' | 'chart' | 'progress' | 'highlight' | 'overlay'
  content: any
  animation: TextAnimation
  position: {
    x: number
    y: number
    width: number
    height: number
  }
  style?: {
    font?: string
    fillStyle?: string
    strokeStyle?: string
    lineWidth?: number
    textAlign?: CanvasTextAlign
  }
}

export interface TextAnimation {
  type: 'typewriter' | 'fadeIn' | 'slideIn' | 'scaleIn' | 'glitch' | 'wave'
  duration: number
  delay: number
  stagger?: number
}

export interface TransitionEffect {
  type: 'crossfade' | 'slide' | 'zoom' | 'wipe' | 'morph' | 'particle'
  duration: number
  direction?: 'left' | 'right' | 'up' | 'down' | 'center'
  easing: string
}

export interface GeneratedVideoResult {
  videoUrl: string
  downloadUrl: string
  thumbnailUrl: string
  duration: number
  segments: VideoSegment[]
  audioUrl?: string
  subtitlesUrl?: string
  originalVideoFrames: string[]
}

class VideoGenerationService {
  private elevenLabsApiKey: string
  private baseConfig: VideoGenerationConfig
  private originalVideoFrames: string[] = []
  private audioDuration: number = 0

  constructor() {
    this.elevenLabsApiKey = import.meta.env.VITE_ELEVENLABS_API_KEY || ''
    this.baseConfig = {
      duration: 60, // Will be dynamic based on voiceover
      width: 1920,
      height: 1080,
      fps: 30,
      voiceSettings: {
        voice: 'Rachel',
        speed: 1.0,
        pitch: 0
      },
      visualStyle: {
        theme: 'modern',
        primaryColor: '#3B82F6',
        secondaryColor: '#8B5CF6',
        fontFamily: 'Inter'
      }
    }
  }

  async generateSummaryVideo(
    videoData: YouTubeVideoData, 
    aiSummary: VideoSummaryData
  ): Promise<GeneratedVideoResult> {
    try {
      console.log('üé¨ Starting enhanced video generation with original content...')
      
      // 1. Extract frames from original video (simulated)
      console.log('üñºÔ∏è Extracting frames from original video...')
      this.originalVideoFrames = await this.extractOriginalVideoFrames(videoData)
      
      // 2. Generate professional voiceover first to determine duration
      console.log('üé§ Generating AI voiceover...')
      const { audioUrl, duration } = await this.generateTimedVoiceover(videoData, aiSummary)
      this.audioDuration = duration
      
      // 3. Create video segments based on actual audio duration
      console.log('üé≠ Creating segments based on voiceover timing...')
      const segments = this.createTimedVideoSegments(videoData, aiSummary, duration)
      
      // 4. Create the video composition with original content
      console.log('üé• Creating video with original frames...')
      const videoResult = await this.createOriginalContentVideo(videoData, aiSummary, segments, audioUrl)
      
      // 5. Generate thumbnail from original video
      console.log('üñºÔ∏è Generating thumbnail...')
      const thumbnailUrl = await this.generateOriginalThumbnail(videoData, aiSummary)
      
      console.log('‚úÖ Enhanced video generation completed!')
      
      return {
        ...videoResult,
        audioUrl,
        thumbnailUrl,
        segments,
        duration: this.audioDuration,
        originalVideoFrames: this.originalVideoFrames
      }
    } catch (error) {
      console.error('‚ùå Enhanced video generation failed:', error)
      return this.generateEnhancedMockVideo(videoData, aiSummary)
    }
  }

  private async extractOriginalVideoFrames(videoData: YouTubeVideoData): Promise<string[]> {
    // In a real implementation, this would extract actual frames from the YouTube video
    // For now, we'll use the thumbnail and create variations
    const frames: string[] = []
    
    try {
      // Use the original thumbnail as base
      frames.push(videoData.thumbnail)
      
      // Generate additional frame URLs (YouTube provides multiple thumbnails)
      const videoId = videoData.id
      const thumbnailVariations = [
        `https://img.youtube.com/vi/${videoId}/maxresdefault.jpg`,
        `https://img.youtube.com/vi/${videoId}/hqdefault.jpg`,
        `https://img.youtube.com/vi/${videoId}/mqdefault.jpg`,
        `https://img.youtube.com/vi/${videoId}/sddefault.jpg`,
        `https://img.youtube.com/vi/${videoId}/0.jpg`,
        `https://img.youtube.com/vi/${videoId}/1.jpg`,
        `https://img.youtube.com/vi/${videoId}/2.jpg`,
        `https://img.youtube.com/vi/${videoId}/3.jpg`
      ]
      
      frames.push(...thumbnailVariations)
      
      console.log(`üì∏ Extracted ${frames.length} frames from original video`)
      return frames
    } catch (error) {
      console.error('‚ùå Failed to extract original video frames:', error)
      return [videoData.thumbnail] // Fallback to just the thumbnail
    }
  }

  private async generateTimedVoiceover(
    videoData: YouTubeVideoData, 
    aiSummary: VideoSummaryData
  ): Promise<{ audioUrl: string; duration: number }> {
    // Create a natural script that reads the key points
    const script = this.createNaturalScript(videoData, aiSummary)
    
    if (!this.elevenLabsApiKey) {
      console.log('üîä No ElevenLabs API key, using enhanced mock audio')
      const mockResult = this.generateTimedMockAudio(script)
      return mockResult
    }

    try {
      console.log('üéôÔ∏è Generating professional AI voiceover...')
      console.log('üìù Script:', script)
      
      const response = await fetch('https://api.elevenlabs.io/v1/text-to-speech/21m00Tcm4TlvDq8ikWAM', {
        method: 'POST',
        headers: {
          'Accept': 'audio/mpeg',
          'Content-Type': 'application/json',
          'xi-api-key': this.elevenLabsApiKey
        },
        body: JSON.stringify({
          text: script,
          model_id: 'eleven_monolingual_v1',
          voice_settings: {
            stability: 0.75,
            similarity_boost: 0.85,
            style: 0.7,
            use_speaker_boost: true
          }
        })
      })

      if (!response.ok) {
        throw new Error(`ElevenLabs API failed: ${response.status}`)
      }

      const audioBlob = await response.blob()
      const audioUrl = URL.createObjectURL(audioBlob)
      
      // Estimate duration based on script length (average speaking rate: 150 words per minute)
      const wordCount = script.split(' ').length
      const estimatedDuration = Math.max(30, Math.min(120, (wordCount / 150) * 60))
      
      console.log(`‚úÖ Professional voiceover generated: ${estimatedDuration}s duration`)
      return { audioUrl, duration: estimatedDuration }
    } catch (error) {
      console.error('‚ùå Voice generation failed:', error)
      return this.generateTimedMockAudio(script)
    }
  }

  private createNaturalScript(videoData: YouTubeVideoData, aiSummary: VideoSummaryData): string {
    const parts = []
    
    // Introduction (natural and engaging)
    parts.push(`Welcome to this AI-powered summary of "${videoData.title}" by ${videoData.channelTitle}.`)
    parts.push(`In the next few minutes, I'll walk you through the ${aiSummary.keyPoints.length} most important insights from this ${Math.floor(videoData.duration / 60)}-minute video about ${aiSummary.mainTopics.slice(0, 2).join(' and ')}.`)
    
    // Key points with natural transitions
    aiSummary.keyPoints.forEach((point, index) => {
      const transitions = [
        `First, let's talk about`,
        `Next, an important point is that`,
        `Another key insight is`,
        `Here's something crucial to understand:`,
        `Additionally, you should know that`,
        `Finally, the last major takeaway is`
      ]
      
      const transition = transitions[Math.min(index, transitions.length - 1)]
      parts.push(`${transition} ${point}.`)
      
      // Add natural pauses
      if (index < aiSummary.keyPoints.length - 1) {
        parts.push('Let me explain this further.')
      }
    })
    
    // Conclusion with call to action
    parts.push(`That covers the main insights from this ${aiSummary.difficulty} level content with a ${aiSummary.sentiment} tone.`)
    parts.push(`The original video goes into much more detail about ${aiSummary.mainTopics.join(', ')}, so I recommend watching the full version for complete understanding.`)
    parts.push('Thanks for watching this AI summary!')
    
    return parts.join(' ')
  }

  private generateTimedMockAudio(script: string): { audioUrl: string; duration: number } {
    // Calculate realistic duration based on script
    const wordCount = script.split(' ').length
    const duration = Math.max(30, Math.min(120, (wordCount / 150) * 60)) // 150 words per minute
    
    try {
      // Create more sophisticated mock audio with speech-like patterns
      const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)()
      const sampleRate = audioContext.sampleRate
      const buffer = audioContext.createBuffer(2, duration * sampleRate, sampleRate)
      
      const leftChannel = buffer.getChannelData(0)
      const rightChannel = buffer.getChannelData(1)

      // Generate speech-like audio patterns
      const sentences = script.split(/[.!?]+/).filter(s => s.trim().length > 0)
      const sentenceDuration = duration / sentences.length
      
      sentences.forEach((sentence, sentenceIndex) => {
        const startSample = Math.floor(sentenceIndex * sentenceDuration * sampleRate)
        const endSample = Math.floor((sentenceIndex + 1) * sentenceDuration * sampleRate)
        
        // Create speech-like frequency patterns
        const words = sentence.trim().split(' ')
        const wordDuration = (endSample - startSample) / words.length
        
        words.forEach((word, wordIndex) => {
          const wordStart = startSample + Math.floor(wordIndex * wordDuration)
          const wordEnd = startSample + Math.floor((wordIndex + 1) * wordDuration)
          
          // Generate formant-like frequencies for speech simulation
          const baseFreq = 100 + Math.random() * 200 // Fundamental frequency
          const formant1 = baseFreq * 3 + Math.random() * 100
          const formant2 = baseFreq * 5 + Math.random() * 200
          
          for (let i = wordStart; i < wordEnd && i < leftChannel.length; i++) {
            const t = i / sampleRate
            const wordProgress = (i - wordStart) / (wordEnd - wordStart)
            
            // Create speech envelope
            const envelope = Math.sin(Math.PI * wordProgress) * 0.1 * (0.7 + Math.random() * 0.3)
            
            // Combine multiple frequencies for speech-like sound
            const fundamental = Math.sin(2 * Math.PI * baseFreq * t) * envelope * 0.4
            const harmonic1 = Math.sin(2 * Math.PI * formant1 * t) * envelope * 0.2
            const harmonic2 = Math.sin(2 * Math.PI * formant2 * t) * envelope * 0.1
            
            const speechSound = fundamental + harmonic1 + harmonic2
            
            leftChannel[i] = speechSound
            rightChannel[i] = speechSound * 0.9 // Slight stereo variation
          }
          
          // Add brief pauses between words
          const pauseStart = wordEnd
          const pauseEnd = Math.min(wordEnd + Math.floor(0.1 * sampleRate), leftChannel.length)
          for (let i = pauseStart; i < pauseEnd; i++) {
            leftChannel[i] = 0
            rightChannel[i] = 0
          }
        })
      })

      // Convert to data URL (simplified)
      console.log(`üîä Generated ${duration}s mock voiceover for ${wordCount} words`)
      return {
        audioUrl: 'data:audio/wav;base64,UklGRnoGAABXQVZFZm10IBAAAAABAAEAQB8AAEAfAAABAAgAZGF0YQoGAACBhYqFbF1fdJivrJBhNjVgodDbq2EcBj+a2/LDciUFLIHO8tiJNwgZaLvt559NEAxQp+PwtmMcBjiR1/LMeSwFJHfH8N2QQAoUXrTp66hVFApGn+DyvmwhBSuBzvLZiTYIG2m98OScTgwOUarm7blmGgU7k9n1unEiBC13yO/eizEIHWq+8+OWT',
        duration
      }
    } catch (error) {
      console.error('‚ùå Enhanced mock audio generation failed:', error)
      return { audioUrl: '', duration: 60 }
    }
  }

  private createTimedVideoSegments(
    videoData: YouTubeVideoData, 
    aiSummary: VideoSummaryData, 
    totalDuration: number
  ): VideoSegment[] {
    const segments: VideoSegment[] = []
    
    // Calculate timing based on content
    const introDuration = Math.min(8, totalDuration * 0.15) // 15% for intro, max 8s
    const conclusionDuration = Math.min(8, totalDuration * 0.15) // 15% for conclusion, max 8s
    const keyPointsTotalDuration = totalDuration - introDuration - conclusionDuration
    const keyPointDuration = keyPointsTotalDuration / aiSummary.keyPoints.length
    
    console.log(`‚è±Ô∏è Video timing: Intro(${introDuration}s) + KeyPoints(${keyPointsTotalDuration}s) + Conclusion(${conclusionDuration}s) = ${totalDuration}s`)
    
    // Intro segment
    segments.push({
      startTime: 0,
      endTime: introDuration,
      type: 'intro',
      content: {
        text: videoData.title,
        voiceText: `Welcome to this AI-powered summary of "${videoData.title}" by ${videoData.channelTitle}.`,
        originalVideoFrame: this.originalVideoFrames[0],
        transitions: [
          { type: 'crossfade', duration: 1.5, easing: 'easeInOut' }
        ],
        visualElements: [
          {
            type: 'text',
            content: {
              title: videoData.title,
              subtitle: `by ${videoData.channelTitle}`,
              badge: `${Math.floor(videoData.duration / 60)}min ‚Üí ${Math.round(totalDuration)}s AI Summary`
            },
            animation: { type: 'slideIn', duration: 2, delay: 0.5 },
            position: { x: 10, y: 20, width: 80, height: 60 },
            style: {
              font: 'bold 64px Inter, sans-serif',
              fillStyle: '#ffffff',
              strokeStyle: '#000000',
              lineWidth: 2,
              textAlign: 'left'
            }
          }
        ]
      }
    })

    // Key Points segments with original video frames
    aiSummary.keyPoints.forEach((point, index) => {
      const startTime = introDuration + (index * keyPointDuration)
      const endTime = startTime + keyPointDuration
      const frameIndex = (index + 1) % this.originalVideoFrames.length

      segments.push({
        startTime,
        endTime,
        type: 'keypoint',
        content: {
          text: point,
          voiceText: point,
          originalVideoFrame: this.originalVideoFrames[frameIndex],
          transitions: [
            { 
              type: index % 2 === 0 ? 'slide' : 'zoom', 
              duration: 1, 
              direction: index % 4 === 0 ? 'left' : index % 4 === 1 ? 'right' : index % 4 === 2 ? 'up' : 'down',
              easing: 'easeInOut' 
            }
          ],
          visualElements: [
            {
              type: 'text',
              content: {
                title: `Key Insight ${index + 1}`,
                content: point,
                topic: aiSummary.mainTopics[index % aiSummary.mainTopics.length]
              },
              animation: { 
                type: index % 3 === 0 ? 'typewriter' : index % 3 === 1 ? 'wave' : 'scaleIn', 
                duration: 1.5, 
                delay: 0.5,
                stagger: 0.05
              },
              position: { x: 5, y: 25, width: 90, height: 50 },
              style: {
                font: 'bold 48px Inter, sans-serif',
                fillStyle: '#ffffff',
                strokeStyle: '#000000',
                lineWidth: 3,
                textAlign: 'left'
              }
            },
            {
              type: 'progress',
              content: { 
                current: index + 1, 
                total: aiSummary.keyPoints.length,
                percentage: ((index + 1) / aiSummary.keyPoints.length) * 100,
                timeRemaining: Math.round(totalDuration - endTime)
              },
              animation: { type: 'slideIn', duration: 0.8, delay: 0.3 },
              position: { x: 10, y: 80, width: 80, height: 6 }
            }
          ]
        }
      })
    })

    // Conclusion segment
    const conclusionStart = totalDuration - conclusionDuration
    segments.push({
      startTime: conclusionStart,
      endTime: totalDuration,
      type: 'conclusion',
      content: {
        text: 'Summary Complete',
        voiceText: `That covers the main insights. The original video goes into much more detail, so I recommend watching the full version. Thanks for watching this AI summary!`,
        originalVideoFrame: this.originalVideoFrames[this.originalVideoFrames.length - 1],
        transitions: [
          { type: 'particle', duration: 2, easing: 'easeOut' }
        ],
        visualElements: [
          {
            type: 'text',
            content: {
              title: 'Summary Complete!',
              subtitle: `${aiSummary.keyPoints.length} key insights covered`,
              cta: 'Watch the full video for complete details'
            },
            animation: { type: 'glitch', duration: 2, delay: 0 },
            position: { x: 10, y: 25, width: 80, height: 50 },
            style: {
              font: 'bold 72px Inter, sans-serif',
              fillStyle: '#22c55e',
              strokeStyle: '#000000',
              lineWidth: 3,
              textAlign: 'center'
            }
          }
        ]
      }
    })

    return segments
  }

  private async createOriginalContentVideo(
    videoData: YouTubeVideoData,
    aiSummary: VideoSummaryData,
    segments: VideoSegment[],
    audioUrl: string
  ): Promise<{ videoUrl: string; downloadUrl: string }> {
    try {
      console.log('üé¨ Creating video with original content and AI voiceover...')
      
      const canvas = document.createElement('canvas')
      canvas.width = 1920
      canvas.height = 1080
      const ctx = canvas.getContext('2d')!

      // Preload original video frames
      const preloadedFrames = await this.preloadOriginalFrames()

      // Set up MediaRecorder for high-quality recording
      const stream = canvas.captureStream(30)
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: 'video/webm;codecs=vp9',
        videoBitsPerSecond: 8000000 // 8 Mbps for high quality
      })

      const chunks: Blob[] = []
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          chunks.push(event.data)
        }
      }

      return new Promise((resolve, reject) => {
        mediaRecorder.onstop = () => {
          const videoBlob = new Blob(chunks, { type: 'video/webm' })
          const videoUrl = URL.createObjectURL(videoBlob)
          console.log('‚úÖ Original content video composition completed')
          resolve({
            videoUrl,
            downloadUrl: videoUrl
          })
        }

        mediaRecorder.onerror = (error) => {
          console.error('‚ùå MediaRecorder error:', error)
          reject(error)
        }

        // Start recording
        mediaRecorder.start()

        // Animate the video with original frames
        this.animateOriginalContentFrames(ctx, videoData, aiSummary, segments, preloadedFrames, () => {
          mediaRecorder.stop()
        })
      })
    } catch (error) {
      console.error('‚ùå Original content video composition failed:', error)
      return this.createContentMatchedMockVideo(videoData, aiSummary)
    }
  }

  private async preloadOriginalFrames(): Promise<HTMLImageElement[]> {
    const loadPromises = this.originalVideoFrames.map(frameUrl => this.loadImage(frameUrl))
    
    try {
      const loadedImages = await Promise.all(loadPromises)
      const validImages = loadedImages.filter(img => img !== null) as HTMLImageElement[]
      console.log(`üì∏ Preloaded ${validImages.length} original video frames`)
      return validImages
    } catch (error) {
      console.error('Failed to preload original frames:', error)
      return []
    }
  }

  private loadImage(url: string): Promise<HTMLImageElement | null> {
    return new Promise((resolve) => {
      const img = new Image()
      img.crossOrigin = 'anonymous'
      
      img.onload = () => resolve(img)
      img.onerror = () => {
        console.warn(`Failed to load frame: ${url}`)
        resolve(null)
      }
      
      // Set timeout for loading
      setTimeout(() => {
        if (!img.complete) {
          resolve(null)
        }
      }, 5000)
      
      img.src = url
    })
  }

  private async animateOriginalContentFrames(
    ctx: CanvasRenderingContext2D,
    videoData: YouTubeVideoData,
    aiSummary: VideoSummaryData,
    segments: VideoSegment[],
    preloadedFrames: HTMLImageElement[],
    onComplete: () => void
  ) {
    const fps = 30
    const totalFrames = this.audioDuration * fps
    let currentFrame = 0
    let previousSegment: VideoSegment | null = null

    const animate = () => {
      const currentTime = currentFrame / fps
      const currentSegment = segments.find(s => currentTime >= s.startTime && currentTime < s.endTime)
      const segmentTime = currentSegment ? currentTime - currentSegment.startTime : 0
      const segmentProgress = currentSegment ? segmentTime / (currentSegment.endTime - currentSegment.startTime) : 0
      
      if (currentSegment) {
        this.renderOriginalContentFrame(
          ctx, 
          currentSegment, 
          previousSegment,
          segmentTime, 
          segmentProgress,
          videoData, 
          aiSummary,
          preloadedFrames
        )
        previousSegment = currentSegment
      }
      
      currentFrame++
      
      if (currentFrame < totalFrames) {
        requestAnimationFrame(animate)
      } else {
        onComplete()
      }
    }

    animate()
  }

  private renderOriginalContentFrame(
    ctx: CanvasRenderingContext2D,
    segment: VideoSegment,
    previousSegment: VideoSegment | null,
    segmentTime: number,
    segmentProgress: number,
    videoData: YouTubeVideoData,
    aiSummary: VideoSummaryData,
    preloadedFrames: HTMLImageElement[]
  ) {
    const { width, height } = ctx.canvas
    
    // Clear canvas
    ctx.clearRect(0, 0, width, height)
    
    // Render background with original video frame
    this.renderOriginalVideoBackground(ctx, segment, previousSegment, segmentTime, segmentProgress, preloadedFrames, { width, height })
    
    // Add content-aware overlay for text readability
    this.addContentAwareOverlay(ctx, aiSummary, segmentProgress, { width, height })
    
    // Render all visual elements with enhanced animations
    segment.content.visualElements.forEach(element => {
      this.renderEnhancedVisualElement(ctx, element, segmentTime, aiSummary, { width, height })
    })
    
    // Add professional UI elements
    this.addProfessionalUIElements(ctx, segment, segmentTime, videoData, aiSummary, { width, height })
  }

  private renderOriginalVideoBackground(
    ctx: CanvasRenderingContext2D,
    segment: VideoSegment,
    previousSegment: VideoSegment | null,
    segmentTime: number,
    segmentProgress: number,
    preloadedFrames: HTMLImageElement[],
    canvasSize: { width: number; height: number }
  ) {
    const currentFrame = this.getFrameForSegment(segment, preloadedFrames)
    const previousFrame = previousSegment ? this.getFrameForSegment(previousSegment, preloadedFrames) : null
    
    // Handle transitions between segments
    if (segmentTime < 1 && previousFrame && segment.content.transitions.length > 0) {
      const transition = segment.content.transitions[0]
      const transitionProgress = segmentTime / transition.duration
      
      this.createImageTransition(
        ctx,
        previousFrame,
        currentFrame,
        transition,
        transitionProgress,
        { x: 0, y: 0, width: canvasSize.width, height: canvasSize.height }
      )
    } else if (currentFrame) {
      // Apply subtle zoom effect for dynamic feel
      this.createParallaxEffect(
        ctx,
        currentFrame,
        { x: 0, y: 0, width: canvasSize.width, height: canvasSize.height },
        segmentProgress,
        0.05
      )
    } else {
      // Fallback gradient background
      this.renderDynamicGradientBackground(ctx, segment, segmentProgress, canvasSize)
    }
    
    // Add subtle overlay for text readability
    ctx.fillStyle = 'rgba(0, 0, 0, 0.4)'
    ctx.fillRect(0, 0, canvasSize.width, canvasSize.height)
  }

  private getFrameForSegment(segment: VideoSegment, preloadedFrames: HTMLImageElement[]): HTMLImageElement | null {
    if (!segment.content.originalVideoFrame || preloadedFrames.length === 0) return null
    
    const frameIndex = this.originalVideoFrames.findIndex(url => url === segment.content.originalVideoFrame)
    return frameIndex >= 0 && frameIndex < preloadedFrames.length ? preloadedFrames[frameIndex] : preloadedFrames[0]
  }

  // Enhanced helper methods for better visual quality
  private createImageTransition(
    ctx: CanvasRenderingContext2D,
    fromImage: HTMLImageElement | null,
    toImage: HTMLImageElement | null,
    transition: TransitionEffect,
    progress: number,
    bounds: { x: number; y: number; width: number; height: number }
  ) {
    const easedProgress = this.easeInOut(progress)
    
    switch (transition.type) {
      case 'crossfade':
        if (fromImage) {
          ctx.save()
          ctx.globalAlpha = 1 - easedProgress
          ctx.drawImage(fromImage, bounds.x, bounds.y, bounds.width, bounds.height)
          ctx.restore()
        }
        if (toImage) {
          ctx.save()
          ctx.globalAlpha = easedProgress
          ctx.drawImage(toImage, bounds.x, bounds.y, bounds.width, bounds.height)
          ctx.restore()
        }
        break
      case 'slide':
        const slideDistance = bounds.width
        if (fromImage) {
          const fromX = bounds.x - slideDistance * easedProgress
          ctx.drawImage(fromImage, fromX, bounds.y, bounds.width, bounds.height)
        }
        if (toImage) {
          const toX = bounds.x + slideDistance * (1 - easedProgress)
          ctx.drawImage(toImage, toX, bounds.y, bounds.width, bounds.height)
        }
        break
      case 'zoom':
        if (fromImage) {
          ctx.save()
          const scale = 1 + easedProgress * 0.2
          ctx.translate(bounds.x + bounds.width/2, bounds.y + bounds.height/2)
          ctx.scale(scale, scale)
          ctx.translate(-bounds.width/2, -bounds.height/2)
          ctx.globalAlpha = 1 - easedProgress
          ctx.drawImage(fromImage, 0, 0, bounds.width, bounds.height)
          ctx.restore()
        }
        if (toImage) {
          ctx.save()
          const scale = 0.8 + easedProgress * 0.2
          ctx.translate(bounds.x + bounds.width/2, bounds.y + bounds.height/2)
          ctx.scale(scale, scale)
          ctx.translate(-bounds.width/2, -bounds.height/2)
          ctx.globalAlpha = easedProgress
          ctx.drawImage(toImage, 0, 0, bounds.width, bounds.height)
          ctx.restore()
        }
        break
    }
  }

  private createParallaxEffect(
    ctx: CanvasRenderingContext2D,
    image: HTMLImageElement,
    bounds: { x: number; y: number; width: number; height: number },
    progress: number,
    intensity: number
  ) {
    const offset = Math.sin(progress * Math.PI * 2) * intensity * bounds.height
    ctx.drawImage(image, bounds.x, bounds.y + offset, bounds.width, bounds.height)
  }

  private easeInOut(t: number): number {
    return t < 0.5 ? 2 * t * t : 1 - Math.pow(-2 * t + 2, 2) / 2
  }

  // Additional helper methods (simplified versions of complex rendering)
  private renderDynamicGradientBackground(ctx: CanvasRenderingContext2D, segment: VideoSegment, progress: number, canvasSize: { width: number; height: number }) {
    const gradient = ctx.createLinearGradient(0, 0, canvasSize.width, canvasSize.height)
    gradient.addColorStop(0, '#1e40af')
    gradient.addColorStop(1, '#7c3aed')
    ctx.fillStyle = gradient
    ctx.fillRect(0, 0, canvasSize.width, canvasSize.height)
  }

  private addContentAwareOverlay(ctx: CanvasRenderingContext2D, aiSummary: VideoSummaryData, progress: number, canvasSize: { width: number; height: number }) {
    // Add subtle animated overlay based on sentiment
    const alpha = 0.1 + Math.sin(progress * Math.PI * 2) * 0.05
    ctx.fillStyle = aiSummary.sentiment === 'positive' ? `rgba(34, 197, 94, ${alpha})` : 
                    aiSummary.sentiment === 'negative' ? `rgba(239, 68, 68, ${alpha})` : 
                    `rgba(59, 130, 246, ${alpha})`
    ctx.fillRect(0, 0, canvasSize.width, canvasSize.height)
  }

  private renderEnhancedVisualElement(ctx: CanvasRenderingContext2D, element: VisualElement, segmentTime: number, aiSummary: VideoSummaryData, canvasSize: { width: number; height: number }) {
    const x = (element.position.x / 100) * canvasSize.width
    const y = (element.position.y / 100) * canvasSize.height
    const w = (element.position.width / 100) * canvasSize.width
    const h = (element.position.height / 100) * canvasSize.height
    
    const progress = Math.max(0, Math.min(1, (segmentTime - element.animation.delay) / element.animation.duration))
    
    if (progress > 0) {
      ctx.save()
      ctx.globalAlpha = progress
      
      if (element.type === 'text') {
        ctx.fillStyle = element.style?.fillStyle || '#ffffff'
        ctx.font = element.style?.font || 'bold 48px Inter, sans-serif'
        ctx.textAlign = element.style?.textAlign || 'left'
        
        if (element.content.title) {
          ctx.fillText(element.content.title, x, y + 50)
        }
        if (element.content.content) {
          ctx.font = '36px Inter, sans-serif'
          this.wrapText(ctx, element.content.content, x, y + 100, w, 50)
        }
      } else if (element.type === 'progress') {
        // Progress bar
        ctx.fillStyle = 'rgba(255, 255, 255, 0.3)'
        ctx.fillRect(x, y, w, h)
        ctx.fillStyle = '#3b82f6'
        ctx.fillRect(x, y, (w * element.content.percentage) / 100, h)
      }
      
      ctx.restore()
    }
  }

  private addProfessionalUIElements(ctx: CanvasRenderingContext2D, segment: VideoSegment, segmentTime: number, videoData: YouTubeVideoData, aiSummary: VideoSummaryData, canvasSize: { width: number; height: number }) {
    // Time indicator
    const currentTime = segment.startTime + segmentTime
    const timeText = `${Math.floor(currentTime)}s / ${Math.round(this.audioDuration)}s`
    
    ctx.fillStyle = 'rgba(0, 0, 0, 0.7)'
    ctx.fillRect(20, canvasSize.height - 60, 120, 40)
    ctx.fillStyle = '#ffffff'
    ctx.font = '18px Inter, sans-serif'
    ctx.textAlign = 'center'
    ctx.fillText(timeText, 80, canvasSize.height - 35)
    
    // AI Summary badge
    ctx.fillStyle = 'rgba(59, 130, 246, 0.8)'
    ctx.fillRect(canvasSize.width - 200, 20, 180, 40)
    ctx.fillStyle = '#ffffff'
    ctx.font = 'bold 16px Inter, sans-serif'
    ctx.textAlign = 'center'
    ctx.fillText('AI SUMMARY', canvasSize.width - 110, 45)
  }

  private wrapText(ctx: CanvasRenderingContext2D, text: string, x: number, y: number, maxWidth: number, lineHeight: number) {
    const words = text.split(' ')
    let line = ''
    let currentY = y

    for (const word of words) {
      const testLine = line + (line ? ' ' : '') + word
      const metrics = ctx.measureText(testLine)
      
      if (metrics.width > maxWidth && line) {
        ctx.fillText(line, x, currentY)
        line = word
        currentY += lineHeight
      } else {
        line = testLine
      }
    }
    
    if (line) {
      ctx.fillText(line, x, currentY)
    }
  }

  private async createContentMatchedMockVideo(videoData: YouTubeVideoData, aiSummary: VideoSummaryData): Promise<{ videoUrl: string; downloadUrl: string }> {
    // Enhanced mock video selection
    const videoUrls = [
      'https://commondatastorage.googleapis.com/gtv-videos-bucket/sample/BigBuckBunny.mp4',
      'https://commondatastorage.googleapis.com/gtv-videos-bucket/sample/ElephantsDream.mp4',
      'https://commondatastorage.googleapis.com/gtv-videos-bucket/sample/ForBiggerBlazes.mp4',
      'https://commondatastorage.googleapis.com/gtv-videos-bucket/sample/ForBiggerEscapes.mp4'
    ]

    const selectedVideo = videoUrls[Math.floor(Math.random() * videoUrls.length)]
    return { videoUrl: selectedVideo, downloadUrl: selectedVideo }
  }

  private generateEnhancedMockVideo(videoData: YouTubeVideoData, aiSummary: VideoSummaryData): GeneratedVideoResult {
    return {
      videoUrl: '',
      downloadUrl: '',
      thumbnailUrl: videoData.thumbnail,
      duration: this.audioDuration || 60,
      segments: this.createTimedVideoSegments(videoData, aiSummary, this.audioDuration || 60),
      audioUrl: '',
      originalVideoFrames: this.originalVideoFrames
    }
  }

  private async generateOriginalThumbnail(videoData: YouTubeVideoData, aiSummary: VideoSummaryData): Promise<string> {
    const canvas = document.createElement('canvas')
    canvas.width = 1280
    canvas.height = 720
    const ctx = canvas.getContext('2d')!

    // Use original video thumbnail as background
    try {
      const img = new Image()
      img.crossOrigin = 'anonymous'
      
      return new Promise((resolve) => {
        img.onload = () => {
          // Draw original thumbnail
          ctx.drawImage(img, 0, 0, canvas.width, canvas.height)
          
          // Add overlay
          ctx.fillStyle = 'rgba(0, 0, 0, 0.5)'
          ctx.fillRect(0, 0, canvas.width, canvas.height)
          
          // Add AI Summary badge
          ctx.fillStyle = '#3b82f6'
          ctx.fillRect(50, 50, 300, 80)
          ctx.fillStyle = '#ffffff'
          ctx.font = 'bold 36px Inter, sans-serif'
          ctx.textAlign = 'left'
          ctx.fillText('AI SUMMARY', 70, 105)
          
          // Add duration badge
          ctx.fillStyle = '#22c55e'
          ctx.fillRect(50, canvas.height - 120, 200, 60)
          ctx.fillStyle = '#ffffff'
          ctx.font = 'bold 24px Inter, sans-serif'
          ctx.textAlign = 'center'
          ctx.fillText(`${Math.round(this.audioDuration)}s`, 150, canvas.height - 85)
          
          resolve(canvas.toDataURL('image/jpeg', 0.95))
        }
        
        img.onerror = () => {
          // Fallback to generated thumbnail
          resolve(this.generateFallbackThumbnail(ctx, videoData, aiSummary))
        }
        
        img.src = videoData.thumbnail
      })
    } catch (error) {
      return this.generateFallbackThumbnail(ctx, videoData, aiSummary)
    }
  }

  private generateFallbackThumbnail(ctx: CanvasRenderingContext2D, videoData: YouTubeVideoData, aiSummary: VideoSummaryData): string {
    const canvas = ctx.canvas
    
    // Create gradient background
    const gradient = ctx.createLinearGradient(0, 0, canvas.width, canvas.height)
    gradient.addColorStop(0, '#1e40af')
    gradient.addColorStop(1, '#7c3aed')
    ctx.fillStyle = gradient
    ctx.fillRect(0, 0, canvas.width, canvas.height)
    
    // Add title
    ctx.fillStyle = '#ffffff'
    ctx.font = 'bold 64px Inter, sans-serif'
    ctx.textAlign = 'center'
    this.wrapText(ctx, videoData.title, canvas.width / 2, 200, canvas.width - 100, 80)
    
    return canvas.toDataURL('image/jpeg', 0.95)
  }
}

export const videoGenerationService = new VideoGenerationService()